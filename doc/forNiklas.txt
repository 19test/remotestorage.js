so the first thing to understand is that the library is not finished. there are still holes here and there. but the basic structure has been layed out. then, there are several parts to understand about how the library works. First of all, the structure. it's the base file (remoteStorage.js), which loads in modules. i have thought about these design decisions for a long time, and i'm very happy with how they worked out, but of course you will end up changing some of them over time. a lot of the important design decisions also come from Jan, ShyByte, binbasti, xMartin, and other people.

The modules have access to baseClient instances, through which they can write to the store. As far as the modules are concerned, that is there whole universe. there is no direct access to the sync. Only one acception, a module can issue a syncNow call to one of its two baseClient instances, which is meant for retrieving a document you don't want to cache. this is not fully implemented yet but the basis is there. the idea is to sync it but set the 'keep' flag to false. when the callback is called, it will be in the store, and can be retrieved from the store the normal way. it will also trigger a change event as normal. Since the 'keep' flag is false, we later know that that resource should be expunged from cache first, should we run out of space. this is not implemented yet but the 'keep' flag is there in the store, and the sync library checks it before it wastes time syncing something that wasn't meant to be kept in cache.

there is another exception which is getCurrentWebRoot. this opens a connection between the module and the fact that we are actually publishing information on the web. I think maybe this is only useful for the public baseClient instance, but it is essential if you want to display an image. You would not hard-code the image into any html template, but you would create an <img> tag on the fly, set its src attribute, and let the browser do its magic (although you can't use bearer tokens there, so even private photos would have to be stored on the public section which is a bit of a bummer).

The modules are meant to provide all the functionality that apps would need around certain types of data. Whenever an app is doing something that is not strictly a visual presentation thing, it should probably go into the appropriate module. this means that you maintaining the modules are basically maintaining most of the important core code of all apps. hopefully people will submit interesting code upstream to you. but from adding the example apps, i could already tell that this is finally the right way to do it. a year ago we tried to unhost apps by syncing localStorage, and we tried with the wireClient (i.e. versions 0.4 - 0.6) but now we finally got it right. you can now unhost an app in 10 minutes, which was always the promise that the approach seemed to have, but that never really came true yet before, somehow.

having said that, app developers you deal with will approach this the wrong way. take into account that they probably have 10 years of  lamp-stack or RoR-stack thinking engrinded in their neural pathways. Even if you explain "there is no server", they will want to have control over the sync.

And even if they accept they don't actually need to have control over the sync, they will want to work without modules.

there will probably also be some trouble with choosing good storage keys. people will want to use one big hash with all the data. That's not a good approach to concurrency and version control. every item that can be changed without affecting other items, should be one object in the storage. that way you can leverage directory listings, and resolve merge conflicts between devices automatically.

it's also important to store the data in the structure in which you will want to retrieve it. and sometimes, a certain way to retrieve the data will just be impossible. for instance, searching across all user accounts of users of some app would require connecting to each user's remoteStorage one-by-one. This is the way nosql works: you have to pick the "disk format" in a way so that you can find it back - the programmer (in this case the module programmer) is responsible for this themselves. Just like with wanting to control the connection to the server because people are used to lamp and RoR, people will want to do random queries because they're used to sql in which you can join tables and execute any query you like. a good example would be if we store 1000 vcards in the contacts category and then say 'oh, now we want to search for Bob'. we should have thought of that before, because if the vcards are just on there without any index or directory structure, we would have to keep all 1000 vcards in sync, and examine them one-by-one in a full table scan every time someone does a lookup. it's not trivial to design the data structures in a future-proof way, but if we discuss each important design choice in module data on the mailing list, i'm sure we'll be fine. so you'll be the gatekeeper of that, but the mailing list is full of smart people who can spot flaws. to me, this has proven the one most valuable asset of our project: a mailing list full of smart people. probably even more valuable than the nlnet funding and the surfnet collaboration.

the widget will need a rethink in the light of the storage-first scenario. i'll work on that with Francois so you don't have to worry much about that. i described its states in /doc/states.svg - there are a lot of states, but most look the same. it's mainly busy (spinning) / idle (just the cube) / anonymous (the login box) that are visually different. most other states look like one of the others. i don't expect you'll have to touch the widget code because Francois and i will probably end up removing it altogether, in favour of the storage-first flow.

webfinger and hardcoded discovery are also part of the widget, and they are also part of what Francois and i will probably be changing.

the base remoteStorage object exposes all the methods the widget uses: setStorageHref, setBearerToken, flushLocal, etcetera. i have been thinking whether the modules should be notified in a special way when a user disconnects. or maybe the app should be notified directly (since it's an action that's global to the app). so probably we need to add notifications for this. so there are the following states in that regard:

- anonymous, no changes (this is always where you start when you first visit an app)
- anonymous, with changes (the user has been using the app, but has not connected yet)
- connected, in sync (everything ok)
- connected, with changes (there is stuff that is either being pushed out, or will be pushed out soon, or will be pushed out when network connectivity is back)

it is important to note that from disconnecting (clicking 'disconnect user@host' in the widget) moves you from 'connected, in sync' to 'anonymous, no changes'. So it removes the connection with your remote account, but also wipes localStorage clean entirely. all data will be gone, until you connect again, and then syncing will start from scratch again.

this is also why clicking disconnect is not allowed in 'connected, with changes' state. now users, and especially developers who are testing something, will constantly click 'disconnect', for instance because you're testing what happens when you reconnect. people who are not chaotic by nature will also have a natural tendency to disconnect when they are finished with something. For instance, Jan deletes all the emails from his inbox after reading them, to keep it clean. Francois powers down his laptop when he's done for the day, and powers it back up in the morning. It's a sense of closure and orderliness. it will surprise them that disconnect means forget everything. but it is the only way that makes sense. if we would leave data behind in localStorage, then people would end up leaving their data behind in internet cafes, and we can't have that. we discussed the option of having two disconnect buttons (hard and soft disconnect) but concluded that there is never a good reason to do a soft disconnect, unless you manage several sessions on the same device, and normal users don't do that. people just log in once with one username, and that's it. so anyway, the way the widget is designed is that people should never click 'disconnect' unless they explicitly want to whipe out their traces from a public computer. maybe we should call it 'whipe out traces' :)

then there's sync. most of the logic for sync is actually built into the 'inode tree' that the store stores in localStorage. right now, it syncs whenever you click the cube. this is for debugging. in production it would set a timer and sync asynchronously once every 60 seconds or so. this timer would (i think) belong in the widget, because it's quite an arbitrary part, and people who want to do their own widget design will probably also want to program their own sync timer. for instance, if there is no activity, then maybe you don't want to spend energy on constantly syncing, whereas when the user is interacting with the tab and moving the mouse, maybe the widget wants to increase the sync frequency.

each inode has the following fields:
 - data (this can be a javascript object, or a data blob, or a directory listing (map of filename -> timestamp); when undefined, the inode represents a file that was deleted but whose metadata is still relevant for syncing)
 - keep (whether we should keep this inode in cache; always true currently)
 - startAccess (when not null, means 'the subtree starting from this node has access 'r' or 'rw' claimed)
 - startForce (when true, means 'this subtree should be fetched proactively and kept in sync with the remote version'. when false, it unsets this for its subtree)
 - outgoingChange (applies only to leafs; false, or local timestamp of when we last changed the data of this inode locally)
 - added (applies only to internal nodes; hash map of items we have added to this directory, but don't exist (yet) remotely)
 - changed (applies only to internal nodes; hash map of items whose data we have changed in this directory, but still have their own data remotely)
 - removed (applies only to internal nodes; hash map of items we have removed from this directory, but still existremotely)
 - lastModified (the last-modified timestamp on remote)
 - lastFetched (local timestamp of when we last fetched the data of this inode)

when syncing, it will traverse the tree hierarchically, and in parallel as far as xhr parallelism is allowed by the browser (most allow 4 concurrent requests per remote domain, i think, so for us that's 4 'xhr threads' in total). if will start at the root, and try to fetch all directories. if it doesn't have access to a certain directories, and the inherited 'force' value is also true for this inode, then it will continue to fetch all its children as defined in the data field of internal (directory) inodes. it will also (it should also do this even if force is not true, actually) traverse the 'added' and 'changed' hashmaps as we want to fetch these nodes to check the current timestamp before writing to them. i actually still have to add the lastFetched field, and check if for instance a certain node was fetched a few seconds ago, it might not be necessary to fetch it again. we could probably allow blind writes if we know that at least 60 seconds ago, the remote value was still unchanged. it is unlikely that a person would be using two device and forget within 60 seconds that they have made a certain change already on the other device. i mean, if i write a=foo on device one and then write a=bar within 60 seconds on device two, then a=bar will win without device two ever knowing that a=foo existed. but obviously, if i change something within 60 seconds, it's because a=foo was the wrong and undesired value. if more time elapses, then i would hope a=foo would have appeared already on device two, so maybe it can warn me 'a already contains value foo, are you sure you want to write bar into it?' (this is effectively app-level conflict resolution). anyway, so it fetches all data and then writes what it has to write. pretty simple actually.


this way, it will fetch all inodes who either:
- fall into a subdir which a module has asked to force syncing for, or
- has local changes, or
- is an ancestor node of an node for which one of the above is true

i think this more or less covers sync.

now about change notifications, this is actually an important part, and the solution we found for this is what allowed us to make sync as (relatively) simple as it is: there are 3 types of change events; window, device, and cloud. each change event contains a .origin field with one of these three values. it also lists path, newValue and oldValue. this is important, because putting the oldValue in the event allows us to already write the new value in the store. if the app doesn't agree, it has access to the old value while handling the event and can change it back.

the reason we're including device-origin and even window-origin events is that it allows to implement the V model. This is, that a click should result in a an event that goes all the way down from presentation to storage layer, and then back from storage layer to presentation layer, traversing all the layers of the mvc architecture in a big V shape (view -> model -> database -> model -> view). We learned this from SproutCore. The good thing about this is that app developers can write 1 handler to handle in coming changes from a module (in the current apps it's all just a whole-screen refreshView(), but you could do something more precision to your DOM if you wanted to.

there are multiple devices and one storage. on each device, the user can have multiple windows open. all windows on one device share localStorage, so if one window makes a change to localStorage, the other windows receive a browser-native StorageEvent. We convert these to device-origin events.

The window that caused the change, does not itself receive a StorageEvent from the browser, but we generate a window-origin event. The app tells the module, the module tells the base client, and the base client triggers the event back up to the module and thorugh there to the app, and also stores the local change in localStorage, which triggers the other windows on the same device to receive the device-origin event, with help of the browser-native StorageEvent.

From this moment on, there is an outgoingChange marked in the store. Next time asynchronous sync passes by, it will result in a PUT. After that, other devices will do their asynchronous sync, and notice the timestamp for one of the items has changed in the ancestor directory. they can follow this trail right down to the file that actually changed (even if this is 5 directory levels down). they will see that the timestamp is newer than the cached copy they have locally, and fetch the resource. they will update their cached copy in store, and if they had themselves any outgoingChange for that same file, if the incoming change is newer than the outgoing one, the outgoing one is discarded. if not, the outgoing one stays pending. for this it is necessary that all devices have their device clocks in sync, but we can think of a way to deal with this if this leads to problems for people.

whether there was an outgoingChange or not, and whether that outgoingChange is discarded or is kept as pending, the incoming change will take effect on the inode in question in localStorage of the receiving device, and this will be passed to the baseclient, and from there to the module in question, and from there to the app, as a cloud-origin change event.

